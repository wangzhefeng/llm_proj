# -*- coding: utf-8 -*-

# ***************************************************
# * File        : steamlit_app_demo.py
# * Author      : Zhefeng Wang
# * Email       : wangzhefengr@163.com
# * Date        : 2024-08-04
# * Version     : 0.1.080417
# * Description : description
# * Link        : https://github.com/datawhalechina/llm-universe/blob/0ce94e828ce2fb63d47741098188544433c5e878/notebook/C4%20%E6%9E%84%E5%BB%BA%20RAG%20%E5%BA%94%E7%94%A8/streamlit_app.py
# * Requirement : ç›¸å…³æ¨¡å—ç‰ˆæœ¬éœ€æ±‚(ä¾‹å¦‚: numpy >= 2.1.0)
# ***************************************************

# python libraries
import os
import sys
ROOT = os.getcwd()
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))
from dotenv import load_dotenv, find_dotenv

# Embedding
from embedding_api.zhipuai_embedding import ZhipuAIEmbeddings
# LLM
from langchain_openai import ChatOpenAI
# Prompt
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
# RAG
from langchain_community.vectorstores import Chroma
from langchain.chains import (
    RetrievalQA,
    ConversationalRetrievalChain
)
from langchain.memory import ConversationBufferMemory
# deploy
import streamlit as st

# global variable
LOGGING_LABEL = __file__.split('/')[-1][:-3]
# å°†çˆ¶ç›®å½•æ”¾å…¥ç³»ç»Ÿè·¯å¾„ä¸­
# sys.path.append("../knowledge_lib")
# è¯»å–æœ¬åœ° .env æ–‡ä»¶
_ = load_dotenv(find_dotenv())
# å¦‚æœéœ€è¦é€šè¿‡ä»£ç†ç«¯å£è®¿é—®ï¼Œä½ éœ€è¦å¦‚ä¸‹é…ç½®
os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'
os.environ["HTTP_PROXY"] = 'http://127.0.0.1:7890'
# è½½å…¥ LLM API KEY
os.environ["OPENAI_API_BASE"] = "https://api.chatgptid.net/v1"
# zhipuai embedding api key
zhipuai_api_key = os.environ["ZHIPUAI_API_KEY"]
# openai llm api key
openai_api_key = os.environ["OPENAI_API_KEY"]


def generate_response(input_text, llm_api_key):
    """
    å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä½¿ç”¨ç”¨æˆ·å¯†é’¥å¯¹ OpenAI API è¿›è¡Œ
        - èº«ä»½éªŒè¯
        - å‘é€æç¤º
        - è·å– AI ç”Ÿæˆçš„å“åº”
    è¯¥å‡½æ•°æ¥å—ç”¨æˆ·çš„æç¤ºä½œä¸ºå‚æ•°ï¼Œå¹¶ä½¿ç”¨ st.info æ¥åœ¨è“è‰²æ¡†ä¸­æ˜¾ç¤º AI ç”Ÿæˆçš„å“åº”ã€‚
    """
    llm = ChatOpenAI(temperature = 0.7, openai_api_key = llm_api_key)
    output = llm.invoke(input_text)
    output_parser = StrOutputParser()
    output = output_parser.invoke(output)
    # TODO st.info(output)
    
    return output


def get_vectordb():
    """
    å‡½æ•°è¿”å›æŒä¹…åŒ–åçš„å‘é‡çŸ¥è¯†åº“
    """
    # å®šä¹‰ Embeddings
    embedding = ZhipuAIEmbeddings()
    # å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
    persist_directory = "E:/projects/llms_proj/llm_proj/app/qa_chain/database/vector_db/chroma"
    # åŠ è½½æ•°æ®åº“
    vectordb = Chroma(
        persist_directory = persist_directory,
        embedding_function = embedding,
    )
    # print(f"å‘é‡åº“ä¸­å­˜å‚¨çš„æ•°é‡ï¼š{vectordb._collection.count()}")

    return vectordb


def get_qa_chain(question: str, llm_api_key: str):
    """
    å‡½æ•°è¿”å›è°ƒç”¨ä¸å¸¦æœ‰å†å²è®°å½•çš„æ£€ç´¢é—®ç­”é“¾åçš„ç»“æœ
    """
    # æŒä¹…åŒ–åçš„å‘é‡çŸ¥è¯†åº“
    vectordb = get_vectordb()
    # Chat LLM
    llm = ChatOpenAI(
        model = "gpt-3.5-turbo",
        temperature = 0,
        opanai_api_key = llm_api_key,
    )
    # Prompt
    template = """"ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œ
    ä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚
    {context}
    é—®é¢˜: {question}
    """
    QA_CHAIN_PROMPT = PromptTemplate(
        input_variables = ["context", "question"],
        template = template,
    )
    # Retrieval QA Chain
    qa_chain = RetrievalQA.from_chain_type(
        llm,
        retriever = vectordb.as_retriever(),
        # è¿”å›æºæ–‡æ¡£ï¼Œé€šè¿‡æŒ‡å®šè¯¥å‚æ•°ï¼Œ
        # å¯ä»¥ä½¿ç”¨ RetrievalQAWithSourceChain() æ–¹æ³•ï¼Œ
        # è¿”å›æºæ–‡æ¡£çš„å¼•ç”¨(åæ ‡æˆ–è€…å«ä¸»é”®ã€ç´¢å¼•)
        return_source_documents = True,
        chain_type_kwargs = {"prompt": QA_CHAIN_PROMPT}
    )
    result = qa_chain({"query": question})

    return result["result"]


def get_chat_qa_chain(question: str, openai_api_key: str):
    """
    å‡½æ•°è¿”å›è°ƒç”¨å¸¦æœ‰å†å²è®°å½•çš„æ£€ç´¢é—®ç­”é“¾åçš„ç»“æœ
    """
    # æŒä¹…åŒ–åçš„å‘é‡çŸ¥è¯†åº“
    vectordb = get_vectordb()
    # Chat LLM
    llm = ChatOpenAI(
        model_name = "gpt-3.5-turbo", 
        temperature = 0, 
        openai_api_key = openai_api_key
    )
    # memory history
    memory = ConversationBufferMemory(
        memory_key = "chat_history",  # ä¸ prompt çš„è¾“å…¥å˜é‡ä¿æŒä¸€è‡´
        return_messages = True,  # å°†æ¶ˆæ¯åˆ—è¡¨çš„å½¢å¼è¿”å›èŠå¤©è®°å½•ï¼Œè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ä¸²
    ) 
    qa = ConversationBufferMemory.from_llm(
        llm, 
        retriever = vectordb.as_retriever(),
        memory = memory,
    )
    result = qa({"question": question})
    
    return result["answer"]


# Streamlit åº”ç”¨ç¨‹åºç•Œé¢
def main():
    # åˆ›å»ºåº”ç”¨ç¨‹åºçš„æ ‡é¢˜
    st.title("ğŸ¦œğŸ”— å‘é‡çŸ¥è¯†åº“-æ£€ç´¢é—®ç­”é“¾-çŸ¥è¯†åº“åŠ©æ‰‹")
    
    # æ·»åŠ ä¸€ä¸ªæ–‡æœ¬è¾“å…¥æ¡†ï¼Œä¾›ç”¨æˆ·è¾“å…¥å…¶ LLM API å¯†é’¥
    llm_api_key = st.sidebar.text_input("OpenAI API Key", type = "password")
    
    # ------------------------------
    # æ·»åŠ ä¸€ä¸ªé€‰æ‹©æŒ‰é’®æ¥é€‰æ‹©ä¸åŒçš„æ¨¡å‹ 
    # ------------------------------
    # selected_method = st.sidebar.selectbox(
    #      "é€‰æ‹©æ¨¡å¼", 
    #      [
    #           "None", 
    #           "qa_chain", 
    #           "chat_qa_chain"
    #      ]
    # )
    selected_method = st.radio(
        "ä½ æƒ³é€‰æ‹©å“ªç§æ¨¡å¼è¿›è¡Œå¯¹è¯ï¼Ÿ",
        [
            "None", 
            "qa_chain", 
            "chat_qa_chain"
        ],
        caption = [
            "ä¸ä½¿ç”¨æ£€ç´¢å›ç­”çš„æ™®é€šæ¨¡å¼", 
            "ä¸å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼", 
            "å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼"
        ]
    )

    # ------------------------------
    # ç”¨äºè·Ÿè¸ªå¯¹è¯å†å² 
    # ------------------------------
    # é€šè¿‡ä½¿ç”¨ st.session_state æ¥å­˜å‚¨å¯¹è¯å†å²ï¼Œå¯ä»¥åœ¨ç”¨æˆ·ä¸åº”ç”¨ç¨‹åºäº¤äº’æ—¶ä¿ç•™æ•´ä¸ªå¯¹è¯çš„ä¸Šä¸‹æ–‡, ç”¨äºè·Ÿè¸ªå¯¹è¯å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # ------------------------------
    # ä½¿ç”¨ st.form() åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡† st.text_area() ä¾›ç”¨æˆ·è¾“å…¥
    # ------------------------------
    # å½“ç”¨æˆ·ç‚¹å‡» Submit æ—¶ï¼Œgenerate_response å°†ä½¿ç”¨ç”¨æˆ·çš„è¾“å…¥ä½œä¸ºå‚æ•°æ¥è°ƒç”¨è¯¥å‡½æ•°
    with st.form("my_form"):
        text = st.text_area(
            "Enter text:", 
            "What are the three key pieces of advice for learning how to code?"
        )
        submitted = st.form_submit_button("Submit")
        
        if not llm_api_key.startswith("sk-"):
            st.warning("Please enter your LLM API key!", icon = "")
        
        if submitted and llm_api_key.startswith("sk-"):
            generate_response(text, llm_api_key)
    
    # ------------------------------
    # 
    # ------------------------------
    messages = st.container(height = 300)
    if prompt := st.chat_input("Say something"):
        # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
        st.session_state.messages.append({
            "role": "user",
            "text": prompt,
        })
        # è°ƒç”¨ respond å‡½æ•°è·å–å›ç­”
        if selected_method == "None":
            answer = generate_response(prompt, llm_api_key)
        if selected_method == "qa_chain":
            answer = get_qa_chain(prompt, llm_api_key)
        elif selected_method == "chat_qa_chain":
            answer = get_chat_qa_chain(prompt, llm_api_key)
        
        # æ£€æŸ¥å›ç­”æ˜¯å¦ä¸º None
        if answer is not None:
            # å°† LLM çš„å›ç­”æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
            st.session_state.messages.append({
                "role": "assistant",
                "text": answer,
            })
        
        # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯å†å²
        for message in st.session_state.messages:
            if message["role"] == "user":
                messages.chat_message("user").write(message["text"])
            else:
                messages.chat_message("assistant").write(message["text"])

if __name__ == "__main__":
    main()
