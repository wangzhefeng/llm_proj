# -*- coding: utf-8 -*-

# ***************************************************
# * File        : chatBot.py
# * Author      : Zhefeng Wang
# * Email       : wangzhefengr@163.com
# * Date        : 2024-08-17
# * Version     : 0.1.081717
# * Description : description
# * Link        : link
# * Requirement : ç›¸å…³æ¨¡å—ç‰ˆæœ¬éœ€æ±‚(ä¾‹å¦‚: numpy >= 2.1.0)
# ***************************************************

# python libraries
import os
import sys
ROOT = os.getcwd()
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import streamlit as st

# global variable
LOGGING_LABEL = __file__.split('/')[-1][:-3]


# åœ¨ä¾§è¾¹æ ä¸­åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªé“¾æ¥
with st.sidebar:
    st.markdown("## LlaMA3.1 LLM")
    "[å¼€æºå¤§æ¨¡å‹æŒ‡å— self-llm](https://github.com/datawhalechina/self-llm.git)"


# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ LLaMA3.1 Chatbot")
st.caption("ğŸš€ A streamlit chatbot powered by Self-LLM")


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œ tokenizer
@st.cache_resource
def get_model():
    # æ¨¡å‹è·¯å¾„
    model_name_or_path = "D:\projects\llms_proj\llm_proj\downloaded_models\LLM-Research\Meta-Llama-3.1-8B-Instruct"
    # ------------------------------
    # åŠ è½½æœ¬åœ° LlaMA-3.1-8B-Instruct æ¨¡å‹
    # ------------------------------
    # åŠ è½½ LlaMA-3.1-8B-Instruct tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name_or_path, 
        # use_fast = False,
        trust_remote_code = True
    )
    tokenizer.pad_token = tokenizer.eos_token

    # åŠ è½½æœ¬åœ° LlaMA3.1-8B-Instruct æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_name_or_path,  
        torch_dtype = torch.bfloat16, 
        # device_map = "auto",
        # trust_remote_code = True,
    ).cuda()

    return tokenizer, model


# åŠ è½½ LlaMA3.1 çš„æ¨¡å‹å’Œ tokenizer
tokenizer, model = get_model()


# å¦‚æœ session_state ä¸­æ²¡æœ‰ "messages"ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªåŒ…å«é»˜è®¤æ¶ˆæ¯çš„åˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# éå† session_state ä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input():
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)
    # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ° session_state ä¸­çš„ messages åˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "user", "content": prompt})
    # å°†å¯¹è¯è¾“å…¥æ¨¡å‹ï¼Œè·å¾—è¿”å›
    input_ids = tokenizer.apply_chat_template(st.session_state["messages"],tokenize=False,add_generation_prompt=True)
    model_inputs = tokenizer([input_ids], return_tensors="pt").to('cuda')
    generated_ids = model.generate(model_inputs.input_ids,max_new_tokens=512)
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ° session_state ä¸­çš„ messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "assistant", "content": response})
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("assistant").write(response)
    print(st.session_state)



# æµ‹è¯•ä»£ç  main å‡½æ•°
def main():
    pass

if __name__ == "__main__":
    main()
